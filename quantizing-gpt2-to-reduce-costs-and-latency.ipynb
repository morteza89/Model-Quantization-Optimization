{"metadata":{"colab":{"collapsed_sections":["btmL4bGy3CmW","SH0cDZiS3Eim","RD8IPp_nPZtR","1DZJ0A6PE-JQ","henH31p4XOht"],"name":"gpt2_quantization","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30684,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Quantizing GPT2 to reduce costs and latency üíµüí™","metadata":{"id":"ztE-GnNZ1y9M"}},{"cell_type":"markdown","source":"## System config ‚öôÔ∏è\nTo install required dependencies","metadata":{"id":"btmL4bGy3CmW"}},{"cell_type":"code","source":"# !pip install -q torch==1.7.0+cpu torchvision==0.8.1+cpu -f https://download.pytorch.org/whl/torch_stable.html\n!pip install -q torch torchvision -f https://download.pytorch.org/whl/torch_stable.html\n# !pip install -q onnxruntime==1.8.0\n!pip install -q transformers==4.3.1 datasets\n!pip install -q onnx onnxconverter_common psutil pytz pandas py-cpuinfo py3nvml coloredlogs","metadata":{"id":"ilCIfc2ZJa9h","_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-04-12T19:33:50.957832Z","iopub.execute_input":"2024-04-12T19:33:50.958413Z","iopub.status.idle":"2024-04-12T19:34:42.090818Z","shell.execute_reply.started":"2024-04-12T19:33:50.958353Z","shell.execute_reply":"2024-04-12T19:34:42.089373Z"},"trusted":true},"execution_count":49,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n  \n  \u001b[31m√ó\u001b[0m \u001b[32mBuilding wheel for tokenizers \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n  \u001b[31m‚îÇ\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n  \u001b[31m‚ï∞‚îÄ>\u001b[0m \u001b[31m[51 lines of output]\u001b[0m\n  \u001b[31m   \u001b[0m running bdist_wheel\n  \u001b[31m   \u001b[0m running build\n  \u001b[31m   \u001b[0m running build_py\n  \u001b[31m   \u001b[0m creating build\n  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-310\n  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-310/tokenizers\n  \u001b[31m   \u001b[0m copying py_src/tokenizers/__init__.py -> build/lib.linux-x86_64-cpython-310/tokenizers\n  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-310/tokenizers/models\n  \u001b[31m   \u001b[0m copying py_src/tokenizers/models/__init__.py -> build/lib.linux-x86_64-cpython-310/tokenizers/models\n  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-310/tokenizers/decoders\n  \u001b[31m   \u001b[0m copying py_src/tokenizers/decoders/__init__.py -> build/lib.linux-x86_64-cpython-310/tokenizers/decoders\n  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-310/tokenizers/normalizers\n  \u001b[31m   \u001b[0m copying py_src/tokenizers/normalizers/__init__.py -> build/lib.linux-x86_64-cpython-310/tokenizers/normalizers\n  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-310/tokenizers/pre_tokenizers\n  \u001b[31m   \u001b[0m copying py_src/tokenizers/pre_tokenizers/__init__.py -> build/lib.linux-x86_64-cpython-310/tokenizers/pre_tokenizers\n  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-310/tokenizers/processors\n  \u001b[31m   \u001b[0m copying py_src/tokenizers/processors/__init__.py -> build/lib.linux-x86_64-cpython-310/tokenizers/processors\n  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-310/tokenizers/trainers\n  \u001b[31m   \u001b[0m copying py_src/tokenizers/trainers/__init__.py -> build/lib.linux-x86_64-cpython-310/tokenizers/trainers\n  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-310/tokenizers/implementations\n  \u001b[31m   \u001b[0m copying py_src/tokenizers/implementations/__init__.py -> build/lib.linux-x86_64-cpython-310/tokenizers/implementations\n  \u001b[31m   \u001b[0m copying py_src/tokenizers/implementations/base_tokenizer.py -> build/lib.linux-x86_64-cpython-310/tokenizers/implementations\n  \u001b[31m   \u001b[0m copying py_src/tokenizers/implementations/byte_level_bpe.py -> build/lib.linux-x86_64-cpython-310/tokenizers/implementations\n  \u001b[31m   \u001b[0m copying py_src/tokenizers/implementations/sentencepiece_bpe.py -> build/lib.linux-x86_64-cpython-310/tokenizers/implementations\n  \u001b[31m   \u001b[0m copying py_src/tokenizers/implementations/char_level_bpe.py -> build/lib.linux-x86_64-cpython-310/tokenizers/implementations\n  \u001b[31m   \u001b[0m copying py_src/tokenizers/implementations/bert_wordpiece.py -> build/lib.linux-x86_64-cpython-310/tokenizers/implementations\n  \u001b[31m   \u001b[0m copying py_src/tokenizers/implementations/sentencepiece_unigram.py -> build/lib.linux-x86_64-cpython-310/tokenizers/implementations\n  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-310/tokenizers/tools\n  \u001b[31m   \u001b[0m copying py_src/tokenizers/tools/__init__.py -> build/lib.linux-x86_64-cpython-310/tokenizers/tools\n  \u001b[31m   \u001b[0m copying py_src/tokenizers/tools/visualizer.py -> build/lib.linux-x86_64-cpython-310/tokenizers/tools\n  \u001b[31m   \u001b[0m copying py_src/tokenizers/__init__.pyi -> build/lib.linux-x86_64-cpython-310/tokenizers\n  \u001b[31m   \u001b[0m copying py_src/tokenizers/models/__init__.pyi -> build/lib.linux-x86_64-cpython-310/tokenizers/models\n  \u001b[31m   \u001b[0m copying py_src/tokenizers/decoders/__init__.pyi -> build/lib.linux-x86_64-cpython-310/tokenizers/decoders\n  \u001b[31m   \u001b[0m copying py_src/tokenizers/normalizers/__init__.pyi -> build/lib.linux-x86_64-cpython-310/tokenizers/normalizers\n  \u001b[31m   \u001b[0m copying py_src/tokenizers/pre_tokenizers/__init__.pyi -> build/lib.linux-x86_64-cpython-310/tokenizers/pre_tokenizers\n  \u001b[31m   \u001b[0m copying py_src/tokenizers/processors/__init__.pyi -> build/lib.linux-x86_64-cpython-310/tokenizers/processors\n  \u001b[31m   \u001b[0m copying py_src/tokenizers/trainers/__init__.pyi -> build/lib.linux-x86_64-cpython-310/tokenizers/trainers\n  \u001b[31m   \u001b[0m copying py_src/tokenizers/tools/visualizer-styles.css -> build/lib.linux-x86_64-cpython-310/tokenizers/tools\n  \u001b[31m   \u001b[0m running build_ext\n  \u001b[31m   \u001b[0m running build_rust\n  \u001b[31m   \u001b[0m error: can't find Rust compiler\n  \u001b[31m   \u001b[0m \n  \u001b[31m   \u001b[0m If you are using an outdated pip version, it is possible a prebuilt wheel is available for this package but pip is not able to install from it. Installing from the wheel would avoid the need for a Rust compiler.\n  \u001b[31m   \u001b[0m \n  \u001b[31m   \u001b[0m To update pip, run:\n  \u001b[31m   \u001b[0m \n  \u001b[31m   \u001b[0m     pip install --upgrade pip\n  \u001b[31m   \u001b[0m \n  \u001b[31m   \u001b[0m and then retry package installation.\n  \u001b[31m   \u001b[0m \n  \u001b[31m   \u001b[0m If you did intend to build this package from source, try installing a Rust compiler from your system package manager and ensure it is on the PATH during installation. Alternatively, rustup (available at https://rustup.rs) is the recommended way to download and update the Rust compiler toolchain.\n  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n  \n  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n\u001b[31m  ERROR: Failed building wheel for tokenizers\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: Could not build wheels for tokenizers, which is required to install pyproject.toml-based projects\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}]},{"cell_type":"code","source":"pip install onnxruntime","metadata":{"execution":{"iopub.status.busy":"2024-04-12T19:34:42.094081Z","iopub.execute_input":"2024-04-12T19:34:42.094469Z","iopub.status.idle":"2024-04-12T19:34:56.960978Z","shell.execute_reply.started":"2024-04-12T19:34:42.094432Z","shell.execute_reply":"2024-04-12T19:34:56.959412Z"},"trusted":true},"execution_count":50,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: onnxruntime in /opt/conda/lib/python3.10/site-packages (1.17.1)\nRequirement already satisfied: coloredlogs in /opt/conda/lib/python3.10/site-packages (from onnxruntime) (15.0.1)\nRequirement already satisfied: flatbuffers in /opt/conda/lib/python3.10/site-packages (from onnxruntime) (23.5.26)\nRequirement already satisfied: numpy>=1.21.6 in /opt/conda/lib/python3.10/site-packages (from onnxruntime) (1.26.4)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from onnxruntime) (21.3)\nRequirement already satisfied: protobuf in /opt/conda/lib/python3.10/site-packages (from onnxruntime) (3.20.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from onnxruntime) (1.12)\nRequirement already satisfied: humanfriendly>=9.1 in /opt/conda/lib/python3.10/site-packages (from coloredlogs->onnxruntime) (10.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->onnxruntime) (3.1.1)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->onnxruntime) (1.3.0)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Imports and general settings üîß","metadata":{"id":"SH0cDZiS3Eim"}},{"cell_type":"code","source":"import torch\nfrom torch import Tensor\nfrom torch.nn import functional as F\nfrom torch.nn import CrossEntropyLoss\nimport os\nfrom onnxruntime.transformers.models.gpt2.gpt2_helper import Gpt2Helper, MyGPT2LMHeadModel\nfrom transformers import AutoConfig\nimport numpy\nfrom transformers import AutoTokenizer\nfrom onnxruntime.transformers.quantize_helper import QuantizeHelper\nfrom onnxruntime import InferenceSession\nfrom transformers import GPT2LMHeadModel, AutoConfig\nimport onnxruntime\nimport numpy as np\nfrom tqdm.notebook import tqdm\nfrom datasets import load_dataset","metadata":{"id":"7lSwHrli4YZR","execution":{"iopub.status.busy":"2024-04-12T19:34:56.963740Z","iopub.execute_input":"2024-04-12T19:34:56.964228Z","iopub.status.idle":"2024-04-12T19:34:56.973669Z","shell.execute_reply.started":"2024-04-12T19:34:56.964183Z","shell.execute_reply":"2024-04-12T19:34:56.972337Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"# In this example we will be quantizing the Dutch GPT2-small model\n\nmodel_ckpt = \"ml6team/gpt2-small-dutch-finetune-oscar\"\ndevice = torch.device(\"cpu\")","metadata":{"id":"c-_reRYJKWEt","execution":{"iopub.status.busy":"2024-04-12T19:34:56.977760Z","iopub.execute_input":"2024-04-12T19:34:56.978209Z","iopub.status.idle":"2024-04-12T19:34:56.988843Z","shell.execute_reply.started":"2024-04-12T19:34:56.978170Z","shell.execute_reply":"2024-04-12T19:34:56.987601Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"\n# Create a cache directory to store pretrained model.\ncache_dir = os.path.join(\".\", \"cache_models\")\nif not os.path.exists(cache_dir):\n    os.makedirs(cache_dir)","metadata":{"id":"6xE4X1EeD_8A","execution":{"iopub.status.busy":"2024-04-12T19:34:56.989880Z","iopub.execute_input":"2024-04-12T19:34:56.990188Z","iopub.status.idle":"2024-04-12T19:34:57.001466Z","shell.execute_reply.started":"2024-04-12T19:34:56.990161Z","shell.execute_reply":"2024-04-12T19:34:57.000219Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"markdown","source":"## Quantization ü§è","metadata":{"id":"LBq8-TvH3GUt"}},{"cell_type":"markdown","source":"### Convert HF model to ONNX","metadata":{"id":"RD8IPp_nPZtR"}},{"cell_type":"code","source":"#Load in the model config\nmodel_name_or_path = model_ckpt\nconfig = AutoConfig.from_pretrained(model_name_or_path, cache_dir=cache_dir)","metadata":{"id":"qQnD7NBv3ziF","execution":{"iopub.status.busy":"2024-04-12T19:34:57.003328Z","iopub.execute_input":"2024-04-12T19:34:57.003647Z","iopub.status.idle":"2024-04-12T19:34:57.120868Z","shell.execute_reply.started":"2024-04-12T19:34:57.003620Z","shell.execute_reply":"2024-04-12T19:34:57.119603Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"# Instantiate the model\nmodel_regular = MyGPT2LMHeadModel.from_pretrained(model_name_or_path, config=config, cache_dir=cache_dir)\n# Activate eval mode to for example deactivate Dropout, and transfer to the device\nmodel_regular.eval().to(device)","metadata":{"id":"jtlWsy6b33vJ","execution":{"iopub.status.busy":"2024-04-12T19:34:57.122832Z","iopub.execute_input":"2024-04-12T19:34:57.123176Z","iopub.status.idle":"2024-04-12T19:34:57.864456Z","shell.execute_reply.started":"2024-04-12T19:34:57.123148Z","shell.execute_reply":"2024-04-12T19:34:57.863257Z"},"trusted":true},"execution_count":55,"outputs":[{"execution_count":55,"output_type":"execute_result","data":{"text/plain":"MyGPT2LMHeadModel(\n  (transformer): GPT2Model(\n    (wte): Embedding(50257, 768)\n    (wpe): Embedding(1024, 768)\n    (drop): Dropout(p=0.1, inplace=False)\n    (h): ModuleList(\n      (0-11): 12 x GPT2Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Conv1D()\n          (c_proj): Conv1D()\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D()\n          (c_proj): Conv1D()\n          (act): NewGELUActivation()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  )\n  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n)"},"metadata":{}}]},{"cell_type":"code","source":"# Obtain parameters for downstream usage\nnum_attention_heads = model_regular.config.n_head\nhidden_size = model_regular.config.n_embd\nnum_layer = model_regular.config.n_layer","metadata":{"id":"b-qSziMH3bia","execution":{"iopub.status.busy":"2024-04-12T19:34:57.870404Z","iopub.execute_input":"2024-04-12T19:34:57.871488Z","iopub.status.idle":"2024-04-12T19:34:57.880531Z","shell.execute_reply.started":"2024-04-12T19:34:57.871445Z","shell.execute_reply":"2024-04-12T19:34:57.878968Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"code","source":"# Export to ONNX binary\nGpt2Helper.export_onnx(model_regular, device, \"gpt2_regular.onnx\")","metadata":{"id":"zd4hQkm1EEbC","execution":{"iopub.status.busy":"2024-04-12T19:34:57.882419Z","iopub.execute_input":"2024-04-12T19:34:57.885052Z","iopub.status.idle":"2024-04-12T19:35:11.604332Z","shell.execute_reply.started":"2024-04-12T19:34:57.885006Z","shell.execute_reply":"2024-04-12T19:35:11.603016Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"markdown","source":"### 1. Optimize before quantization\nTo for example perform step fusing in the model graph","metadata":{"id":"1DZJ0A6PE-JQ"}},{"cell_type":"code","source":"Gpt2Helper.optimize_onnx(\n    \"gpt2_regular.onnx\",\n    \"gpt2_regular_opt.onnx\",\n    False,\n    model_regular.config.num_attention_heads,\n    model_regular.config.hidden_size)","metadata":{"id":"y6nXYluK5Pvu","execution":{"iopub.status.busy":"2024-04-12T19:35:11.611125Z","iopub.execute_input":"2024-04-12T19:35:11.611528Z","iopub.status.idle":"2024-04-12T19:35:28.923114Z","shell.execute_reply.started":"2024-04-12T19:35:11.611498Z","shell.execute_reply":"2024-04-12T19:35:28.922243Z"},"trusted":true},"execution_count":58,"outputs":[{"execution_count":58,"output_type":"execute_result","data":{"text/plain":"<onnx_model_gpt2.Gpt2OnnxModel at 0x793da19dbfa0>"},"metadata":{}}]},{"cell_type":"markdown","source":"### 2. Quantize the models","metadata":{"id":"VKYdr9Bb51mT"}},{"cell_type":"code","source":"QuantizeHelper.quantize_onnx_model(\n    \"gpt2_regular_opt.onnx\",\n    \"gpt2_regular_opt_int8.onnx\")","metadata":{"id":"4RQQyeAV56kk","execution":{"iopub.status.busy":"2024-04-12T19:35:28.924243Z","iopub.execute_input":"2024-04-12T19:35:28.925111Z","iopub.status.idle":"2024-04-12T19:35:38.317258Z","shell.execute_reply.started":"2024-04-12T19:35:28.925080Z","shell.execute_reply":"2024-04-12T19:35:38.315974Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"markdown","source":"## Evaluate the quantized model üîé","metadata":{"id":"7byDq3ZJQQRb"}},{"cell_type":"markdown","source":"### Sampling code\nSince we don't want to perform greedy decoding, but use a more sophisticated sampling strategy, we had to **coughs* borrow some code from [this HF repo page](https://github.com/huggingface/transformers/blob/main/src/transformers/generation_utils.py).","metadata":{"id":"xObR--ZA-vIA"}},{"cell_type":"code","source":"def top_k_top_p_filtering(\n    logits: Tensor,\n    top_k: int = 0,\n    top_p: float = 1.0,\n    filter_value: float = -float(\"Inf\"),\n    min_tokens_to_keep: int = 1,\n) -> Tensor:\n    \"\"\"Filter a distribution of logits using top-k and/or nucleus (top-p) filtering\n    Args:\n        logits: logits distribution shape (batch size, vocabulary size)\n        if top_k > 0: keep only top k tokens with highest probability (top-k filtering).\n        if top_p < 1.0: keep the top tokens with cumulative probability >= top_p (nucleus filtering).\n            Nucleus filtering is described in Holtzman et al. (http://arxiv.org/abs/1904.09751)\n        Make sure we keep at least min_tokens_to_keep per batch example in the output\n    From: https://gist.github.com/thomwolf/1a5a29f6962089e871b94cbd09daf317\n    \"\"\"\n    if top_k > 0:\n        top_k = min(max(top_k, min_tokens_to_keep), logits.size(-1))  # Safety check\n        # Remove all tokens with a probability less than the last token of the top-k\n        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n        logits[indices_to_remove] = filter_value\n\n    if top_p < 1.0:\n        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n\n        # Remove tokens with cumulative probability above the threshold (token with 0 are kept)\n        sorted_indices_to_remove = cumulative_probs > top_p\n        if min_tokens_to_keep > 1:\n            # Keep at least min_tokens_to_keep (set to min_tokens_to_keep-1 because we add the first one below)\n            sorted_indices_to_remove[..., :min_tokens_to_keep] = 0\n        # Shift the indices to the right to keep also the first token above the threshold\n        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n        sorted_indices_to_remove[..., 0] = 0\n\n        # scatter sorted tensors to original indexing\n        indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)\n        logits[indices_to_remove] = filter_value\n    return logits","metadata":{"id":"YKlMzlGIEY90","execution":{"iopub.status.busy":"2024-04-12T19:35:38.322601Z","iopub.execute_input":"2024-04-12T19:35:38.323024Z","iopub.status.idle":"2024-04-12T19:35:38.339000Z","shell.execute_reply.started":"2024-04-12T19:35:38.322985Z","shell.execute_reply":"2024-04-12T19:35:38.337915Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"markdown","source":"### Inference helper code\nThese helper methods have been copied from [this notebook](https://github.com/microsoft/onnxruntime/blob/master/onnxruntime/python/tools/transformers/notebooks/Inference_GPT2_with_OnnxRuntime_on_CPU.ipynb) from the ONNXRuntime github repo.","metadata":{"id":"Ppg-8P7FQwM1"}},{"cell_type":"code","source":"def get_tokenizer(model_name_or_path, cache_dir):\n    # Fetch and prepare the tokenizer\n    tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, cache_dir=cache_dir)\n    tokenizer.padding_side = \"left\"\n    tokenizer.pad_token = tokenizer.eos_token\n    #okenizer.add_special_tokens({'pad_token': '[PAD]'})\n    return tokenizer\n\ndef get_example_inputs(prompt_text):  \n    # Prepare the input text for furhter processing  \n    tokenizer = get_tokenizer(model_name_or_path, cache_dir)\n    encodings_dict = tokenizer.batch_encode_plus(prompt_text, padding=True)\n\n    input_ids = torch.tensor(encodings_dict['input_ids'], dtype=torch.int64)\n    attention_mask = torch.tensor(encodings_dict['attention_mask'], dtype=torch.float32)\n    position_ids = (attention_mask.long().cumsum(-1) - 1)\n    position_ids.masked_fill_(position_ids < 0, 0)\n\n    #Empty Past State for generating first word\n    empty_past = []\n    batch_size = input_ids.size(0)\n    sequence_length = input_ids.size(1)\n    past_shape = [2, batch_size, num_attention_heads, 0, hidden_size // num_attention_heads]\n    for i in range(num_layer):\n        empty_past.append(torch.empty(past_shape).type(torch.float32).to(device))\n       \n    return input_ids, attention_mask, position_ids, empty_past","metadata":{"id":"gRfFQRCx6V00","execution":{"iopub.status.busy":"2024-04-12T19:35:38.340544Z","iopub.execute_input":"2024-04-12T19:35:38.341264Z","iopub.status.idle":"2024-04-12T19:35:38.357241Z","shell.execute_reply.started":"2024-04-12T19:35:38.341233Z","shell.execute_reply":"2024-04-12T19:35:38.356117Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"code","source":"def regular_inference_with_io_binding(session, config, input_ids, position_ids, attention_mask, past):\n    # Helper method to perform ORT session inference with IO Binding\n    output_shapes = Gpt2Helper.get_output_shapes(batch_size=input_ids.size(0),\n                                                 past_sequence_length=past[0].size(3),\n                                                 sequence_length=input_ids.size(1),\n                                                 config=config)\n    output_buffers = Gpt2Helper.get_output_buffers(output_shapes, device)\n\n    io_binding = Gpt2Helper.prepare_io_binding(session, input_ids, position_ids, attention_mask, past,\n                                               output_buffers, output_shapes)\n    session.run_with_iobinding(io_binding)\n\n    outputs = Gpt2Helper.get_outputs_from_io_binding_buffer(session, output_buffers, output_shapes,\n                                                            return_numpy=False)\n    return outputs","metadata":{"id":"cH7llKIjhuAb","execution":{"iopub.status.busy":"2024-04-12T19:35:38.358932Z","iopub.execute_input":"2024-04-12T19:35:38.359384Z","iopub.status.idle":"2024-04-12T19:35:38.370267Z","shell.execute_reply.started":"2024-04-12T19:35:38.359325Z","shell.execute_reply":"2024-04-12T19:35:38.369375Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"code","source":"def regular_test_generation(tokenizer, input_text, ort_session=None, num_tokens_to_produce = 30, top_k=50, top_p=0.95, do_sample=False, temperature=1.0):\n    use_onnxruntime = (ort_session is not None)\n    print(\"Text generation using\", \"OnnxRuntime\" if use_onnxruntime else \"PyTorch\", \"...\")\n    eos_token_id = tokenizer.eos_token_id\n    \n    input_ids, attention_mask, position_ids, past = get_example_inputs(input_text)\n    batch_size = input_ids.size(0)\n\n    has_eos = torch.zeros(batch_size, dtype=torch.bool)\n\n    all_token_ids = input_ids.clone()\n\n    for step in range(num_tokens_to_produce):\n        outputs = regular_inference_with_io_binding(ort_session, config, input_ids, position_ids, attention_mask, past)\n\n        # Get next logits\n        next_token_logits = outputs[0][:, -1, :]\n\n        # Top-k sampling\n        if do_sample:\n            # Temperature (higher temperature => more likely to sample low probability tokens)\n            if temperature != 1.0:\n                    scores = next_token_logits / temperature\n            next_token_logscores = top_k_top_p_filtering(scores, top_k=top_k, top_p=top_p)\n            probs = F.softmax(next_token_logscores, dim=-1)\n            next_tokens = torch.multinomial(probs, num_samples=1).squeeze(1)\n        else:\n            # Greedy sampling\n            next_tokens = torch.argmax(next_token_logits, dim=-1)\n\n        has_eos = has_eos | (next_tokens == eos_token_id)\n        tokens_to_add = next_tokens.masked_fill(has_eos, eos_token_id)\n        all_token_ids = torch.cat([all_token_ids, tokens_to_add.unsqueeze(-1)], dim=-1)\n\n        # Update input_ids, attention_mask, position_ids and past\n        input_ids = tokens_to_add.clone().detach().reshape([batch_size, 1]).to(device)    \n        position_ids = (position_ids[:,-1] + 1).reshape(batch_size,1)\n        attention_mask = torch.cat([attention_mask, torch.ones([batch_size, 1]).type_as(attention_mask)], 1).to(device)    \n\n        past = []\n        if not use_onnxruntime:\n            past = list(outputs[1]) # past in torch output is tuple\n        else:\n            for i in range(num_layer):\n                past_i = torch.from_numpy(outputs[i + 1]) if isinstance(outputs[i + 1], numpy.ndarray) else outputs[i + 1].clone().detach()\n                past.append(past_i.to(device))\n\n        if torch.all(has_eos):\n            break\n\n    for i, output in enumerate(all_token_ids):\n        print(\"------------\")\n        print(tokenizer.decode(output, skip_special_tokens=True))","metadata":{"id":"rw6QIocxJXg5","execution":{"iopub.status.busy":"2024-04-12T19:35:38.371982Z","iopub.execute_input":"2024-04-12T19:35:38.372672Z","iopub.status.idle":"2024-04-12T19:35:38.390852Z","shell.execute_reply.started":"2024-04-12T19:35:38.372638Z","shell.execute_reply":"2024-04-12T19:35:38.389827Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"markdown","source":"### Basic output quality tests","metadata":{"id":"mEpO9ChpR-5-"}},{"cell_type":"code","source":"tokenizer = get_tokenizer(model_name_or_path, cache_dir)\nlength=5","metadata":{"id":"hWsfzmqa6vRy","execution":{"iopub.status.busy":"2024-04-12T19:35:38.392419Z","iopub.execute_input":"2024-04-12T19:35:38.392783Z","iopub.status.idle":"2024-04-12T19:35:38.734317Z","shell.execute_reply.started":"2024-04-12T19:35:38.392754Z","shell.execute_reply":"2024-04-12T19:35:38.732930Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"code","source":"session_int8_regular = InferenceSession(\"gpt2_regular_opt_int8.onnx\")","metadata":{"id":"8R3MmCzH6ov_","execution":{"iopub.status.busy":"2024-04-12T19:35:38.736419Z","iopub.execute_input":"2024-04-12T19:35:38.736907Z","iopub.status.idle":"2024-04-12T19:35:39.336094Z","shell.execute_reply.started":"2024-04-12T19:35:38.736872Z","shell.execute_reply":"2024-04-12T19:35:39.334639Z"},"trusted":true},"execution_count":65,"outputs":[]},{"cell_type":"code","source":"input_text = ['Dit is een test om', 'Dit is een test om', 'Dit is een test om']","metadata":{"id":"MXzhvNbwSM18","execution":{"iopub.status.busy":"2024-04-12T19:35:39.340343Z","iopub.execute_input":"2024-04-12T19:35:39.340856Z","iopub.status.idle":"2024-04-12T19:35:39.679391Z","shell.execute_reply.started":"2024-04-12T19:35:39.340813Z","shell.execute_reply":"2024-04-12T19:35:39.678388Z"},"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"code","source":"regular_test_generation(\n    tokenizer,\n    input_text,\n    do_sample=True,\n    top_p=0.95,\n    top_k=50,\n    temperature=0.95,\n    ort_session=session_int8_regular,\n    num_tokens_to_produce=length)","metadata":{"id":"wqV9Fqz-7DR8","execution":{"iopub.status.busy":"2024-04-12T19:35:39.681140Z","iopub.execute_input":"2024-04-12T19:35:39.681486Z","iopub.status.idle":"2024-04-12T19:35:40.191089Z","shell.execute_reply.started":"2024-04-12T19:35:39.681457Z","shell.execute_reply":"2024-04-12T19:35:40.190164Z"},"trusted":true},"execution_count":67,"outputs":[{"name":"stdout","text":"Text generation using OnnxRuntime ...\n------------\nDit is een test om belangrijke eigenschap van de mens\n------------\nDit is een test om en de het de en\n------------\nDit is een test om echte bijzonder manier grootste ultieme\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Compare the output logits","metadata":{"id":"c5tC8kXD1ki1"}},{"cell_type":"code","source":"model_name_or_path= model_ckpt","metadata":{"id":"Zy8QUV2r1w2r","execution":{"iopub.status.busy":"2024-04-12T19:35:40.193138Z","iopub.execute_input":"2024-04-12T19:35:40.193601Z","iopub.status.idle":"2024-04-12T19:35:40.199437Z","shell.execute_reply.started":"2024-04-12T19:35:40.193558Z","shell.execute_reply":"2024-04-12T19:35:40.198238Z"},"trusted":true},"execution_count":68,"outputs":[]},{"cell_type":"code","source":"input_ids, attention_mask, position_ids, empty_past = get_example_inputs(prompt_text=[\"Ik zie het niet meer zitten om\"])","metadata":{"id":"duGgTGH_2CBo","execution":{"iopub.status.busy":"2024-04-12T19:35:40.201528Z","iopub.execute_input":"2024-04-12T19:35:40.202446Z","iopub.status.idle":"2024-04-12T19:35:40.506628Z","shell.execute_reply.started":"2024-04-12T19:35:40.202399Z","shell.execute_reply":"2024-04-12T19:35:40.505633Z"},"trusted":true},"execution_count":69,"outputs":[]},{"cell_type":"markdown","source":"#### Testing dataset\nFor testing, we will select a small sample from the OSCAR dutch corpus","metadata":{"id":"1f8O8Cr-TDZm"}},{"cell_type":"code","source":"dataset = load_dataset(\"nthngdy/oscar-mini\", \"unshuffled_deduplicated_nl\", download_mode=\"force_redownload\")","metadata":{"id":"7Ba5WKcF3858","execution":{"iopub.status.busy":"2024-04-12T19:35:40.508616Z","iopub.execute_input":"2024-04-12T19:35:40.508966Z","iopub.status.idle":"2024-04-12T19:35:55.043916Z","shell.execute_reply.started":"2024-04-12T19:35:40.508938Z","shell.execute_reply":"2024-04-12T19:35:55.042509Z"},"trusted":true},"execution_count":70,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/12.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f569739d8283475895b7419216c95167"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/13.4k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"47cefb663b144b3f870b156922982710"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/22.7M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9e203c9a10fb4d5d8cc3808af986d4fe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/193602 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"773830c4953f4798a127658277849b9d"}},"metadata":{}}]},{"cell_type":"markdown","source":"#### HF logits","metadata":{"id":"3tO4P1WI2r2_"}},{"cell_type":"code","source":"config = AutoConfig.from_pretrained(model_name_or_path, cache_dir=cache_dir)\n\ntorch_model = GPT2LMHeadModel.from_pretrained(model_name_or_path, config=config, cache_dir=cache_dir)\ndevice = torch.device(\"cpu\")\ntorch_model.eval().to(device)","metadata":{"id":"l_N03oxu14AV","execution":{"iopub.status.busy":"2024-04-12T19:35:55.047366Z","iopub.execute_input":"2024-04-12T19:35:55.047872Z","iopub.status.idle":"2024-04-12T19:35:55.806932Z","shell.execute_reply.started":"2024-04-12T19:35:55.047826Z","shell.execute_reply":"2024-04-12T19:35:55.805993Z"},"trusted":true},"execution_count":71,"outputs":[{"execution_count":71,"output_type":"execute_result","data":{"text/plain":"GPT2LMHeadModel(\n  (transformer): GPT2Model(\n    (wte): Embedding(50257, 768)\n    (wpe): Embedding(1024, 768)\n    (drop): Dropout(p=0.1, inplace=False)\n    (h): ModuleList(\n      (0-11): 12 x GPT2Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Conv1D()\n          (c_proj): Conv1D()\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D()\n          (c_proj): Conv1D()\n          (act): NewGELUActivation()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  )\n  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n)"},"metadata":{}}]},{"cell_type":"code","source":"def get_hf_logits(input_ids, empty_past, attention_mask, position_ids):\n    with torch.no_grad():\n        torch_output = torch_model(input_ids, past_key_values=empty_past, attention_mask=attention_mask, position_ids=position_ids)\n    \n    return torch_output[0]","metadata":{"id":"oJxLn_4-1nrI","execution":{"iopub.status.busy":"2024-04-12T19:35:55.808059Z","iopub.execute_input":"2024-04-12T19:35:55.810225Z","iopub.status.idle":"2024-04-12T19:35:55.816528Z","shell.execute_reply.started":"2024-04-12T19:35:55.810187Z","shell.execute_reply":"2024-04-12T19:35:55.814892Z"},"trusted":true},"execution_count":72,"outputs":[]},{"cell_type":"markdown","source":"#### ORT logits","metadata":{"id":"wNiqSFAa2vb_"}},{"cell_type":"code","source":"onnx_model_path = \"gpt2_regular_opt_int8.onnx\"\nsession = onnxruntime.InferenceSession(onnx_model_path)\n\ndef get_ort_logits(input_ids, empty_past, attention_mask, position_ids):\n    ort_inputs = {'input_ids': np.ascontiguousarray(input_ids.cpu().numpy()),\n                'attention_mask' : np.ascontiguousarray(attention_mask.cpu().numpy()),\n                'position_ids': np.ascontiguousarray(position_ids.cpu().numpy())\n                }\n    for i, past_i in enumerate(empty_past):\n#         ort_inputs[f'past_{i}'] = np.ascontiguousarray(past_i.cpu().numpy())\n        ort_inputs[f'past_{i}'] = np.ascontiguousarray(past_i.to(torch.float).cpu().numpy())\n\n    ort_outputs = session.run(None, ort_inputs)\n\n    return ort_outputs[0]","metadata":{"id":"bj8sAEqk2t6Z","execution":{"iopub.status.busy":"2024-04-12T19:35:55.818242Z","iopub.execute_input":"2024-04-12T19:35:55.818739Z","iopub.status.idle":"2024-04-12T19:35:56.703503Z","shell.execute_reply.started":"2024-04-12T19:35:55.818696Z","shell.execute_reply":"2024-04-12T19:35:56.702153Z"},"trusted":true},"execution_count":73,"outputs":[]},{"cell_type":"markdown","source":"#### Compare","metadata":{"id":"YjT7b1sT278D"}},{"cell_type":"code","source":"all_max_logits_diff = []\nall_mean_logits_diff = []\nall_median_logits_diff = []\n\nfor line in tqdm(dataset['train'][\"text\"][:100]):\n\n    # get inputs\n    input_ids, attention_mask, position_ids, empty_past = get_example_inputs(prompt_text=[line])\n\n    # hf logits\n    hf_logits = get_hf_logits(input_ids, empty_past, attention_mask, position_ids)\n\n    # ort logits\n    input_ids = input_ids.to(torch.int32)\n    attention_mask = attention_mask.to(torch.int32)\n    position_ids = position_ids.to(torch.int32)\n    \n    empty_past = [tensor.to(torch.int32) for tensor in empty_past]\n    ort_logits = get_ort_logits(input_ids, empty_past, attention_mask, position_ids)\n\n    # compare\n    logits_masked_diff = (hf_logits - ort_logits) * attention_mask.unsqueeze(2)\n\n    max_logits_diff = logits_masked_diff.abs().max()\n    mean_logits_diff = logits_masked_diff.abs().mean()\n    median_logits_diff = logits_masked_diff.abs().median()\n\n    all_max_logits_diff.append(max_logits_diff)\n    all_mean_logits_diff.append(mean_logits_diff)\n    all_median_logits_diff.append(median_logits_diff)","metadata":{"id":"ZnvCUujM_Qw8","execution":{"iopub.status.busy":"2024-04-12T19:35:56.705480Z","iopub.execute_input":"2024-04-12T19:35:56.705834Z","iopub.status.idle":"2024-04-12T19:37:10.724639Z","shell.execute_reply.started":"2024-04-12T19:35:56.705796Z","shell.execute_reply":"2024-04-12T19:37:10.723350Z"},"trusted":true},"execution_count":74,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/100 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1e78aebf96c5456ab613935789b33833"}},"metadata":{}}]},{"cell_type":"code","source":"print(np.mean(all_max_logits_diff))\nprint(np.mean(all_mean_logits_diff))\nprint(np.mean(all_median_logits_diff))","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GDFDlMFX_-3t","outputId":"51dd4526-9025-46b8-b06a-d85c3431cbf7","execution":{"iopub.status.busy":"2024-04-12T19:37:10.726606Z","iopub.execute_input":"2024-04-12T19:37:10.727699Z","iopub.status.idle":"2024-04-12T19:37:10.740467Z","shell.execute_reply.started":"2024-04-12T19:37:10.727653Z","shell.execute_reply":"2024-04-12T19:37:10.738851Z"},"trusted":true},"execution_count":75,"outputs":[{"name":"stdout","text":"16.297436\n2.9655762\n2.4627645\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Perplexity\nTo measure the text generation capabilities","metadata":{"id":"Walu7I8g30H6"}},{"cell_type":"code","source":"#Fetch a portion of the test dataset\n\ntotal_string= \"\\n\\n\".join(dataset['train'][\"text\"][:1000])\nencodings = tokenizer(total_string, return_tensors=\"pt\")\n\ninput_ids, attention_mask, position_ids, empty_past = get_example_inputs(prompt_text=[total_string])","metadata":{"id":"dFW8gaUS4OX7","execution":{"iopub.status.busy":"2024-04-12T19:37:10.748732Z","iopub.execute_input":"2024-04-12T19:37:10.749238Z","iopub.status.idle":"2024-04-12T19:37:12.164841Z","shell.execute_reply.started":"2024-04-12T19:37:10.749201Z","shell.execute_reply":"2024-04-12T19:37:12.163678Z"},"trusted":true},"execution_count":76,"outputs":[]},{"cell_type":"code","source":"max_length = torch_model.config.n_positions\nstride = 512","metadata":{"id":"2EVQi6-SXNSL","execution":{"iopub.status.busy":"2024-04-12T19:37:12.171489Z","iopub.execute_input":"2024-04-12T19:37:12.171902Z","iopub.status.idle":"2024-04-12T19:37:12.180192Z","shell.execute_reply.started":"2024-04-12T19:37:12.171871Z","shell.execute_reply":"2024-04-12T19:37:12.178712Z"},"trusted":true},"execution_count":77,"outputs":[]},{"cell_type":"markdown","source":"#### Torch model PPL","metadata":{"id":"henH31p4XOht"}},{"cell_type":"code","source":"nlls = []\n\nfor i in tqdm(range(0, input_ids.size(1), stride)):\n    begin_loc = max(i + stride - max_length, 0)\n    end_loc = min(i + stride, input_ids.size(1))\n    trg_len = end_loc - i  # may be different from stride on last loop\n    input_ids_local = input_ids[:, begin_loc:end_loc].to(device)\n    target_ids = input_ids_local.clone()\n    target_ids[:, :-trg_len] = -100\n\n    with torch.no_grad():\n        outputs = torch_model(input_ids_local, labels=target_ids)\n        neg_log_likelihood = outputs[0] * trg_len\n\n    nlls.append(neg_log_likelihood)\n\ntorch_ppl = torch.exp(torch.stack(nlls).sum() / end_loc)","metadata":{"id":"soAyUpkJ3zeM","execution":{"iopub.status.busy":"2024-04-12T19:37:12.182360Z","iopub.execute_input":"2024-04-12T19:37:12.182859Z","iopub.status.idle":"2024-04-12T19:43:25.291005Z","shell.execute_reply.started":"2024-04-12T19:37:12.182812Z","shell.execute_reply":"2024-04-12T19:43:25.289623Z"},"trusted":true},"execution_count":78,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/121 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2ad645d8906f4fc7af6baaef2cde1a48"}},"metadata":{}}]},{"cell_type":"markdown","source":"#### ORT quantized PPL","metadata":{"id":"moN17nG9XvsV"}},{"cell_type":"code","source":"onnx_model_path = \"gpt2_regular_opt_int8.onnx\"\nsession = onnxruntime.InferenceSession(onnx_model_path)\n\nnlls = []\nfor i in tqdm(range(0, input_ids.size(1), stride)):\n    begin_loc = max(i + stride - max_length, 0)\n    end_loc = min(i + stride, input_ids.size(1))\n    trg_len = end_loc - i  # may be different from stride on last loop\n\n    # Slice it up\n    input_ids_local = input_ids[:, begin_loc:end_loc].to(device)\n    attention_mask_local = attention_mask[:, begin_loc:end_loc].to(device)\n    position_ids_local = position_ids[:,:input_ids_local.size(1)].to(device)\n\n    target_ids = input_ids_local.clone()\n    target_ids[:, :-trg_len] = -100\n    \n    # ort logits\n    input_ids_local = input_ids_local.to(torch.int32)\n    attention_mask_local = attention_mask_local.to(torch.int32)\n    position_ids_local = position_ids_local.to(torch.int32)\n    \n    empty_past = [tensor.to(torch.int32) for tensor in empty_past]\n\n    ort_inputs = {\n        'input_ids': numpy.ascontiguousarray(input_ids_local.cpu().numpy()),\n        'attention_mask' : numpy.ascontiguousarray(attention_mask_local.cpu().numpy()),\n        'position_ids': numpy.ascontiguousarray(position_ids_local.cpu().numpy())\n        }\n\n    for i, past_i in enumerate(empty_past):\n#         ort_inputs[f'past_{i}'] = numpy.ascontiguousarray(past_i.cpu().numpy())\n        ort_inputs[f'past_{i}'] = np.ascontiguousarray(past_i.to(torch.float).cpu().numpy())\n\n    \n    \n    ort_outputs = session.run(None, ort_inputs)\n    ort_outputs_logits = torch.from_numpy(ort_outputs[0])\n\n    # Calculate loss\n\n    shift_logits = ort_outputs_logits[..., :-1, :].contiguous()\n    shift_labels = target_ids[..., 1:].contiguous()\n    \n    shift_logits = ort_outputs_logits[..., :-1, :].contiguous()\n    # Ensure shift_labels is of type torch.long\n    shift_labels = target_ids[..., 1:].contiguous().to(torch.long)\n\n    loss_fct = CrossEntropyLoss()\n    loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n\n    neg_log_likelihood = loss * trg_len\n\n    # print(neg_log_likelihood)\n\n    nlls.append(neg_log_likelihood)\n\nquantized_ppl = torch.exp(torch.stack(nlls).sum() / end_loc)","metadata":{"id":"VB3cpmxJ7Fw4","execution":{"iopub.status.busy":"2024-04-12T19:43:25.293389Z","iopub.execute_input":"2024-04-12T19:43:25.293882Z","iopub.status.idle":"2024-04-12T19:47:57.947548Z","shell.execute_reply.started":"2024-04-12T19:43:25.293839Z","shell.execute_reply":"2024-04-12T19:47:57.946338Z"},"trusted":true},"execution_count":79,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/121 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eccea0ce4e734a2783a77a5aff3a37db"}},"metadata":{}}]},{"cell_type":"markdown","source":"#### Compare","metadata":{"id":"ZJ_zA7cmYATQ"}},{"cell_type":"code","source":"print(f\"Non-quantized perplexity: {torch_ppl}\")\nprint(f\"Quantized perplexity: {quantized_ppl}\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IUschPH7LP7U","outputId":"ecbd197f-b917-4e15-c259-d8b140d1d245","execution":{"iopub.status.busy":"2024-04-12T19:47:57.949253Z","iopub.execute_input":"2024-04-12T19:47:57.949660Z","iopub.status.idle":"2024-04-12T19:47:57.957223Z","shell.execute_reply.started":"2024-04-12T19:47:57.949628Z","shell.execute_reply":"2024-04-12T19:47:57.955886Z"},"trusted":true},"execution_count":80,"outputs":[{"name":"stdout","text":"Non-quantized perplexity: 52.99111557006836\nQuantized perplexity: 77.47394561767578\n","output_type":"stream"}]}]}